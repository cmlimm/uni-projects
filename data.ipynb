{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ничего из нижеизложенного можно не запускать, так как далее есть кнопки для загрузки сразу нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['1SorcerersStone.txt', \n",
    "             '2ChamberofSecrets.txt',\n",
    "             '3ThePrisonerOfAzkaban.txt',\n",
    "             '4TheGobletOfFire.txt',\n",
    "             '5OrderofthePhoenix.txt',\n",
    "             '6TheHalfBloodPrince.txt',\n",
    "             '7DeathlyHollows.txt']\n",
    "with open('harrypotter.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_text_filename = 'harrypotter.txt'\n",
    "text_filename = 'harrypotter_cleaned.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_filename, 'w') as file_write:\n",
    "    with open(initial_text_filename, \"r\") as file_read:\n",
    "        for line in file_read:\n",
    "            # удаляем окаймляющие кавычки из строк\n",
    "            line = line.strip('\"\"')\n",
    "            \n",
    "            # удаляем имена собственные\n",
    "            names = re.compile(r'[A-Z][a-z]+')\n",
    "            line = re.sub(names, '', line)\n",
    "            \n",
    "            # удаляем артикли\n",
    "            names = re.compile(r\"'|’\")\n",
    "            line = re.sub(names, '', line)\n",
    "            \n",
    "            # разделяем строки на логические составляющие по знакам препинания\n",
    "            marks = re.compile(r'\\.|\\:|;|\\?|!|--|\"')\n",
    "            line = re.sub(marks, ':SPLIT:', line)\n",
    "            \n",
    "            # удаляем запятые\n",
    "            comma = re.compile(r\",\")\n",
    "            line = re.sub(comma, '', line)\n",
    "            \n",
    "            # удаляем тире\n",
    "            comma = re.compile(r\"-\")\n",
    "            line = re.sub(comma, ' ', line)\n",
    "            \n",
    "            line = line.strip('\\n')\n",
    "            lines = line.split(':SPLIT:')\n",
    "            for line in lines:\n",
    "                # убираем возможные окаймляющие пробелы\n",
    "                line = line.strip(' ')\n",
    "                # разделяем строку на слова\n",
    "                line = line.split(' ')\n",
    "                # каждое слово переводим в нижний регистр\n",
    "                line = list(map(lambda word: word.lower(), line))\n",
    "                # если строка не состоит из одного слова то записываем её в файл\n",
    "                if len(line) != 1:\n",
    "                    line = ' '.join(line)+'\\n'\n",
    "                    file_write.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество слов с повторениями:  925975\n",
      "Количество уникальных слов: 21071\n",
      "Пример из списка всех слов: rry\n",
      "Пример из списка всех предложений: ['rry', 'and', 'the', 's']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "all_sent = []\n",
    "with open(text_filename, 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "        all_sent += [words]\n",
    "\n",
    "print(\"Количество слов с повторениями: \", len(all_words))\n",
    "all_words_norep = list(set(all_words))\n",
    "print(\"Количество уникальных слов:\", len(all_words_norep))\n",
    "print(\"Пример из списка всех слов:\", all_words[0])\n",
    "print(\"Пример из списка всех предложений:\", all_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение списка всех слов в файл\n",
    "with open('all_words_norep.json', 'w') as file:\n",
    "    json.dump(all_words_norep, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words = {}\n",
    "for word in all_words_norep:\n",
    "    next_words[word] = {}\n",
    "    \n",
    "next_words['acting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'as': 1,\n",
       " 'him': 3,\n",
       " 'it': 3,\n",
       " 'to': 15,\n",
       " 'potion': 9,\n",
       " 'those': 1,\n",
       " 'at': 1,\n",
       " 'not': 1,\n",
       " 'yeh': 1,\n",
       " 'since': 1,\n",
       " 'lives': 1,\n",
       " 'with': 8,\n",
       " 'of': 3,\n",
       " 'hearing': 1,\n",
       " 'this': 1,\n",
       " 'life': 1,\n",
       " 'them': 1,\n",
       " 'a': 1,\n",
       " 'for': 2,\n",
       " 'me': 2,\n",
       " 'knitting': 1,\n",
       " 'stuff': 1,\n",
       " 'potions': 4,\n",
       " 'being': 1,\n",
       " 'porion': 1,\n",
       " 'and': 4,\n",
       " 'her': 4,\n",
       " 'my': 1,\n",
       " 'is': 1,\n",
       " 'you': 3,\n",
       " 'in': 1,\n",
       " 'if': 1,\n",
       " 'the': 1,\n",
       " 'loyalty': 1,\n",
       " 'again': 1,\n",
       " 'which': 1,\n",
       " 'did': 1,\n",
       " 'that': 1,\n",
       " 'some': 1}"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in all_sent:\n",
    "    n_words = len(sentence)\n",
    "    for word_ind in range(n_words):\n",
    "        current_word = sentence[word_ind]\n",
    "        # если слово не последнее в строке, то увеличиваем количество раз, \n",
    "        # которое следующее слово встретилось после текущего\n",
    "        if word_ind != n_words - 1:\n",
    "            next_word = sentence[word_ind + 1]\n",
    "            if next_word in next_words[current_word]:\n",
    "                next_words[current_word][next_word] += 1\n",
    "            else:\n",
    "                next_words[current_word][next_word] = 1\n",
    "\n",
    "next_words['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_words = {}\n",
    "for word in all_words_norep:\n",
    "    prev_words[word] = {}\n",
    "    \n",
    "prev_words['acting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1,\n",
       " 'that': 2,\n",
       " 'll': 1,\n",
       " 'id': 10,\n",
       " 'a': 6,\n",
       " 'their': 1,\n",
       " 'really': 1,\n",
       " 'd': 3,\n",
       " 'found': 1,\n",
       " 'i': 9,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'tangled': 1,\n",
       " 'in': 10,\n",
       " 'the': 2,\n",
       " 'hed': 3,\n",
       " 'surely': 1,\n",
       " 's': 3,\n",
       " 'you': 2,\n",
       " 'no': 1,\n",
       " 'doesnt': 1,\n",
       " 'who': 2,\n",
       " 'do': 1,\n",
       " 'powerful': 1,\n",
       " 'create': 1,\n",
       " 'imitate': 1,\n",
       " 'obsessive': 1,\n",
       " 'theyd': 1,\n",
       " 'would': 1,\n",
       " 'unrequited': 1,\n",
       " 'different': 1,\n",
       " 'bring': 1,\n",
       " 'strong': 1,\n",
       " 'with': 1,\n",
       " 'undying': 1,\n",
       " 'disappointed': 1,\n",
       " 'swallowed': 1,\n",
       " 'can': 2,\n",
       " 'just': 1,\n",
       " 'not': 3,\n",
       " 'more': 1,\n",
       " 'my': 1,\n",
       " 'youd': 1,\n",
       " 'isnt': 1,\n",
       " 'was': 1,\n",
       " 'her': 2,\n",
       " 'and': 1,\n",
       " 'without': 1,\n",
       " 'it': 1,\n",
       " 'though': 1,\n",
       " 'solution': 1,\n",
       " 'great': 1,\n",
       " 'our': 1,\n",
       " 'professor': 1,\n",
       " 'him': 1}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in all_sent:\n",
    "    n_words = len(sentence)\n",
    "    for word_ind in range(n_words - 1, 0, -1):\n",
    "        current_word = sentence[word_ind]\n",
    "        # если слово не последнее в строке, то увеличиваем количество раз, \n",
    "        # которое следующее слово встретилось после текущего\n",
    "        prev_word = sentence[word_ind - 1]\n",
    "        if prev_word in prev_words[current_word]:\n",
    "            prev_words[current_word][prev_word] += 1\n",
    "        else:\n",
    "            prev_words[current_word][prev_word] = 1\n",
    "\n",
    "prev_words['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'as': 0.011494252873563218,\n",
       " 'him': 0.034482758620689655,\n",
       " 'it': 0.034482758620689655,\n",
       " 'to': 0.1724137931034483,\n",
       " 'potion': 0.10344827586206896,\n",
       " 'those': 0.011494252873563218,\n",
       " 'at': 0.011494252873563218,\n",
       " 'not': 0.011494252873563218,\n",
       " 'yeh': 0.011494252873563218,\n",
       " 'since': 0.011494252873563218,\n",
       " 'lives': 0.011494252873563218,\n",
       " 'with': 0.09195402298850575,\n",
       " 'of': 0.034482758620689655,\n",
       " 'hearing': 0.011494252873563218,\n",
       " 'this': 0.011494252873563218,\n",
       " 'life': 0.011494252873563218,\n",
       " 'them': 0.011494252873563218,\n",
       " 'a': 0.011494252873563218,\n",
       " 'for': 0.022988505747126436,\n",
       " 'me': 0.022988505747126436,\n",
       " 'knitting': 0.011494252873563218,\n",
       " 'stuff': 0.011494252873563218,\n",
       " 'potions': 0.04597701149425287,\n",
       " 'being': 0.011494252873563218,\n",
       " 'porion': 0.011494252873563218,\n",
       " 'and': 0.04597701149425287,\n",
       " 'her': 0.04597701149425287,\n",
       " 'my': 0.011494252873563218,\n",
       " 'is': 0.011494252873563218,\n",
       " 'you': 0.034482758620689655,\n",
       " 'in': 0.011494252873563218,\n",
       " 'if': 0.011494252873563218,\n",
       " 'the': 0.011494252873563218,\n",
       " 'loyalty': 0.011494252873563218,\n",
       " 'again': 0.011494252873563218,\n",
       " 'which': 0.011494252873563218,\n",
       " 'did': 0.011494252873563218,\n",
       " 'that': 0.011494252873563218,\n",
       " 'some': 0.011494252873563218}"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in next_words:\n",
    "    total = sum(next_words[word].values())\n",
    "    for next_word in next_words[word]:\n",
    "        next_words[word][next_word] = next_words[word][next_word]/total\n",
    "        \n",
    "next_words['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.009523809523809525,\n",
       " 'that': 0.01904761904761905,\n",
       " 'll': 0.009523809523809525,\n",
       " 'id': 0.09523809523809523,\n",
       " 'a': 0.05714285714285714,\n",
       " 'their': 0.009523809523809525,\n",
       " 'really': 0.009523809523809525,\n",
       " 'd': 0.02857142857142857,\n",
       " 'found': 0.009523809523809525,\n",
       " 'i': 0.08571428571428572,\n",
       " 'to': 0.02857142857142857,\n",
       " 'of': 0.0380952380952381,\n",
       " 'tangled': 0.009523809523809525,\n",
       " 'in': 0.09523809523809523,\n",
       " 'the': 0.01904761904761905,\n",
       " 'hed': 0.02857142857142857,\n",
       " 'surely': 0.009523809523809525,\n",
       " 's': 0.02857142857142857,\n",
       " 'you': 0.01904761904761905,\n",
       " 'no': 0.009523809523809525,\n",
       " 'doesnt': 0.009523809523809525,\n",
       " 'who': 0.01904761904761905,\n",
       " 'do': 0.009523809523809525,\n",
       " 'powerful': 0.009523809523809525,\n",
       " 'create': 0.009523809523809525,\n",
       " 'imitate': 0.009523809523809525,\n",
       " 'obsessive': 0.009523809523809525,\n",
       " 'theyd': 0.009523809523809525,\n",
       " 'would': 0.009523809523809525,\n",
       " 'unrequited': 0.009523809523809525,\n",
       " 'different': 0.009523809523809525,\n",
       " 'bring': 0.009523809523809525,\n",
       " 'strong': 0.009523809523809525,\n",
       " 'with': 0.009523809523809525,\n",
       " 'undying': 0.009523809523809525,\n",
       " 'disappointed': 0.009523809523809525,\n",
       " 'swallowed': 0.009523809523809525,\n",
       " 'can': 0.01904761904761905,\n",
       " 'just': 0.009523809523809525,\n",
       " 'not': 0.02857142857142857,\n",
       " 'more': 0.009523809523809525,\n",
       " 'my': 0.009523809523809525,\n",
       " 'youd': 0.009523809523809525,\n",
       " 'isnt': 0.009523809523809525,\n",
       " 'was': 0.009523809523809525,\n",
       " 'her': 0.01904761904761905,\n",
       " 'and': 0.009523809523809525,\n",
       " 'without': 0.009523809523809525,\n",
       " 'it': 0.009523809523809525,\n",
       " 'though': 0.009523809523809525,\n",
       " 'solution': 0.009523809523809525,\n",
       " 'great': 0.009523809523809525,\n",
       " 'our': 0.009523809523809525,\n",
       " 'professor': 0.009523809523809525,\n",
       " 'him': 0.009523809523809525}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in prev_words:\n",
    "    total = sum(prev_words[word].values())\n",
    "    for prev_word in prev_words[word]:\n",
    "        prev_words[word][prev_word] = prev_words[word][prev_word]/total\n",
    "        \n",
    "prev_words['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение словарей в файл\n",
    "with open('next_words.json', 'w') as file:\n",
    "    json.dump(next_words, file)\n",
    "    \n",
    "with open('prev_words.json', 'w') as file:\n",
    "    json.dump(prev_words, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не запускать, считается долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подсчитываем частоту встречаемости каждого слова\n",
    "words_count = {}\n",
    "for word in all_words_norep:\n",
    "    words_count[word] = all_words.count(word)\n",
    "    \n",
    "words_count['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011555387564459084"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_prob = words_count.copy()\n",
    "all_words_n = len(all_words)\n",
    "for word in words_prob:\n",
    "    words_prob[word] = words_prob[word]/all_words_n\n",
    "    \n",
    "words_prob['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение словарей в файл\n",
    "with open('words_count.json', 'w') as file:\n",
    "    json.dump(words_count, file)\n",
    "\n",
    "with open('words_prob.json', 'w') as file:\n",
    "    json.dump(words_prob,## Загрузка нужных файлов file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выгрузка словарей\n",
    "with open('words_count.json', 'r') as file:\n",
    "    words_count = json.load(file)\n",
    "\n",
    "with open('words_prob.json', 'r') as file:\n",
    "    words_prob = json.load(file)\n",
    "    \n",
    "# загрузка словарей\n",
    "with open('next_words.json', 'r') as file:\n",
    "    next_words = json.load(file)\n",
    "    \n",
    "with open('prev_words.json', 'r') as file:\n",
    "    prev_words = json.load(file)\n",
    "    \n",
    "with open('all_words_norep.json', 'r') as file:\n",
    "    all_words_norep = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все далее изложенное запускать можно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.390321964043864"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, inf\n",
    "\n",
    "def bayes(previous, word, neighbours, words_dict):\n",
    "    # складываем вероятность слова с вероятностями соседствования (до или после) с предыдущими словами\n",
    "    prob = log(words_dict[word])\n",
    "    count = 0\n",
    "    for prev in previous:\n",
    "        try:\n",
    "            prob += log(neighbours[word][prev])\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    if count == 0:\n",
    "        return -inf\n",
    "    else:\n",
    "        # идея в том, чтобы вероятность не становилась сильно меньше, если предыдущих слов много\n",
    "        return prob/count\n",
    "\n",
    "bayes(['i', 'love'], 'but', next_words, words_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "# получаем топ-эн предсказаний следующего слова\n",
    "# есть возможность учесть, что слово начинается с каких-то букв\n",
    "def top_n(previous, n, words_list, neighbours, words_dict, beginning=''):\n",
    "    words_list = filter(lambda word: word.startswith(beginning.lower()), words_list)\n",
    "    top = nlargest(n, words_list, key=lambda word: bayes(previous, word, neighbours, words_dict))\n",
    "    \n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'for', 'in', 'of', 'like']\n"
     ]
    }
   ],
   "source": [
    "print(top_n('brew a potion'.split(), 5, all_words_norep, next_words, words_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'that', 'and', 'though', 'but']\n"
     ]
    }
   ],
   "source": [
    "print(top_n('he hates'.split(), 5, all_words_norep, next_words, words_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moment', 'me', 'mean', 'maybe', 'much']\n"
     ]
    }
   ],
   "source": [
    "print(top_n('he hates'.split(), 5, all_words_norep, next_words, words_prob, 'm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расстояние Хэмминга\n",
    "def hamming_distance(word1, word2):\n",
    "    n1 = len(word1)\n",
    "    n2 = len(word2)\n",
    "    \n",
    "    return sum(c1 != c2 for c1, c2 in zip(word1, word2))+abs(n2 - n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция аналогична подсказке слова, но вместо этого\n",
    "# исправляет текущее слово с учетом расстояния Хэмминга и вероятностей\n",
    "def top_n_mistakes(previous, n, mistaken, words_list, neighbours, words_dict):\n",
    "    top = nlargest(n, words_list, \n",
    "                   key=lambda word: bayes(previous, word, neighbours, words_dict)*hamming_distance(mistaken, word))\n",
    "    \n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'as', 'my', 'but', 'if']\n"
     ]
    }
   ],
   "source": [
    "print(top_n_mistakes('he hates'.split(), 5, 'mu', all_words_norep, next_words, words_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive(phrase, dictionary, found = []):\n",
    "    \"\"\"\n",
    "    Функция для разделения фразы на все возможные слова рекурсивно\n",
    "\n",
    "    phrase: фраза без пробелов\n",
    "    dictionary: словарь\n",
    "    found: уже найденные слова из разбиения\n",
    "    \"\"\"\n",
    "    first_letter = 0\n",
    "    n = len(phrase)\n",
    "    sentences = []\n",
    "\n",
    "    # если фраза кончилась, то выходим из рекурсии\n",
    "    if n == 0:\n",
    "        return found\n",
    "\n",
    "    # начинаем идти с конца фразы\n",
    "    for last_letter in range(n, first_letter, -1):\n",
    "        # если слово от текущей первой буквы до последней есть в словаре\n",
    "        # то добавляем его\n",
    "        if phrase[first_letter:last_letter] in dictionary:\n",
    "            found = found + [phrase[first_letter:last_letter]]\n",
    "            # запускаем рекурсию от меньшей фразы, то есть убираем слово, которое\n",
    "            # только что нашли\n",
    "            rec = recursive(phrase[last_letter:], dictionary, found)\n",
    "            sentences += [rec]\n",
    "            # убираем только что найденное слово, чтобы найти другие варианты\n",
    "            # разбиения\n",
    "            found = found[:-1]\n",
    "        # если осталась только одна буква, то добавляем её\n",
    "        elif last_letter - first_letter == 1:\n",
    "            found = found + [phrase[first_letter:last_letter]]\n",
    "            rec = recursive(phrase[last_letter:], dictionary, found)\n",
    "            sentences += [rec]\n",
    "            found = found[:-1]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def remove_nested(sentences):\n",
    "    \"\"\"\n",
    "    Функция убирает вложенность из списка списков путем перевода в строку\n",
    "\n",
    "    sentences: изначальный список предложений\n",
    "    \"\"\"\n",
    "    sentences = str(sentences)\n",
    "    # убираем лишние открывающие и закрывающие скобки\n",
    "    sentences = re.sub(r'\\[+', '[', sentences)\n",
    "    sentences = re.sub(r'\\]+', ']', sentences)\n",
    "    sentences = re.sub(r'\\], \\[', ']\\n[', sentences)\n",
    "    sentences = sentences.split('\\n')\n",
    "\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        sentence = [word.strip(\"[''],\") for word in sentence.split()]\n",
    "        result.append(sentence)\n",
    "\n",
    "    return result\n",
    "\n",
    "def sentence_log_probability(sentence, dictionary):\n",
    "    prob = 0\n",
    "    for word in sentence:\n",
    "        prob += log(dictionary[word])\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sent(sentence, words):\n",
    "    recur = remove_nested(recursive('hewasalive', words))\n",
    "    best = max(recur, key=lambda sent: sentence_log_probability(sent, words))\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'was', 'alive']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sent('hewasalive', words_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
