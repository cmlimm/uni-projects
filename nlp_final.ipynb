{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing | Задачи обработки естественного языка\n",
    "    > токенизация\n",
    "    > определение частей речи слов\n",
    "    > извлечение специальных названий\n",
    "    > синтаксический разбор предложений\n",
    "    > анализ тональности текста\n",
    "    > определение основных тем текста\n",
    "    > векторизация слов и предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможности библиотеки:\n",
    "+ разбиение текста на предложения\n",
    "+ разбиение текста на слова\n",
    "+ разбиение предложений на слова\n",
    "+ определение частей речи предложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Разбиение текста на предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"NLTK has been called a wonderful tool for teaching, working in computational linguistics using Python. NLTK is an amazing library to play with natural language. NLTK is a leading platform for building Python programs to work with human language data. NLTL is really useful.\"\n",
    "nltk.sent_tokenize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение текста на слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Отчищаем данные от знаков препинания и стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [i for i in nltk.word_tokenize(text1) if i not in string.punctuation and i not in stopwords.words('english')]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение слов с наибольшими частотами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = nltk.FreqDist(word_list)\n",
    "freq_list.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение текста на предложения и слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Natural Language Processing is manipulation or understanding text or speech by any software or machine. An analogy is that humans interact, understand each other views, and respond with the appropriate answer. In NLP, this interaction, understanding, the response is made by a computer instead of a human.\"\n",
    "s = [nltk.word_tokenize(i) for i in nltk.sent_tokenize(text2)]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение частей речи слов предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag_sents(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Теги:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение основы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\") \n",
    "stemmer.stem(\"Василий\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический анализатор pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Возможности библиотеки:\n",
    " + морфологический разбор слов\n",
    " + нахождение инфинитивов слов\n",
    " + приведение слов к нужной форме\n",
    " + нахождение всех форм слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Морфологический разбор слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(parse):\n",
    "    print(parse.word, ' -> ', parse.normal_form, parse.tag.cyr_repr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in morph.parse('стали'):\n",
    "    show(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение начальной формы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = morph.parse('бегала')[0]\n",
    "word.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " word.tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращение к атрибутам тегов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> атрибуты помогают получить информацию о части речи, числе и других характеристиках слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.tag.POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.tag.gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.tag.tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'VERB' in word.tag.POS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение всех форм слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_forms(parsers):\n",
    "    lst=[]\n",
    "    for p in parsers:\n",
    "        lst.append(p.word)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_forms(morph.parse('спать')[0].lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласование слов с числительными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = morph.parse('апельсин')[0]\n",
    "w.make_agree_with_number(5).word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дан текст отзыва покупателя, используя библиотеку nltk, проанализируйте характер отзыва"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In my head, a review about the M. Video store chain has long been ripening. Since the very time when I again faced with the problem of returning money for a certificate of the Additional Services Program. I already wrote a review about the work of Eldorado, in fact, faced with the same problem, but for the first time. True, in Eldorado this lure is called the Additional Service Program, but the difference is small. Actually, to this day I think that M-video is a bad store, but due to the small selection of those in the city, sometimes I still get something there. pah-pah, while it works. However, today I want to share my opinion about M. Video. I will tell you about all the problems of this place. This is the second largest electronics store that we have in the city. A wide range of products is presented here, although it is often impossible to find a specific model. The store has promotions, discounts, a system of accumulative bonuses, credit purchases and other rubbish, which is called a service. However, there are many problems with all promotions. Once again, we got into such a bad situation. Firstly, the service in the store was terrible. And the quality of the goods was bad. Two years ago, they threw themselves off, threw themselves in and presented mom with a new TV set that had broken. It was very disappointing that the TV broke down, since we paid a huge amount for it. Not a single visit ended without a problem. However, when they bought Dad, they again drew this one and imposed the acquisition of a certificate of the Additional Services Program. So roughly (this is a copy) this stucco looks and costs decent money. The attraction is that at the end of the certificate validity period, money for unused services will be returned. But all this lies, with a certificate you get even more problems! Do not believe in stocks and do not go to stores with a bad reputation, do not create problems for yourself!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя библиотеки nltk и pymorphy2, определите, какую части речи в стихах чаще всего употребляют \n",
    "в стихах следующие авторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pushkin = '''\n",
    "Мороз и солнце; день чудесный!\n",
    "Еще ты дремлешь, друг прелестный —\n",
    "Пора, красавица, проснись:\n",
    "Открой сомкнуты негой взоры\n",
    "Навстречу северной Авроры,\n",
    "Звездою севера явись!\n",
    "\n",
    "Вечор, ты помнишь, вьюга злилась,\n",
    "На мутном небе мгла носилась;\n",
    "Луна, как бледное пятно,\n",
    "Сквозь тучи мрачные желтела,\n",
    "И ты печальная сидела —\n",
    "А нынче погляди в окно:\n",
    "\n",
    "Под голубыми небесами\n",
    "Великолепными коврами,\n",
    "Блестя на солнце, снег лежит;\n",
    "Прозрачный лес один чернеет,\n",
    "И ель сквозь иней зеленеет,\n",
    "И речка подо льдом блестит.\n",
    "'''\n",
    "\n",
    "Mayakovsky = '''\n",
    "Послушайте!\n",
    "Ведь, если звезды зажигают —\n",
    "значит — это кому-нибудь нужно?\n",
    "Значит — кто-то хочет, чтобы они были?\n",
    "Значит — кто-то называет эти плевочки\n",
    "\n",
    "жемчужиной?\n",
    "И, надрываясь\n",
    "в метелях полуденной пыли,\n",
    "врывается к богу,\n",
    "боится, что опоздал,\n",
    "плачет,\n",
    "целует ему жилистую руку,\n",
    "просит —\n",
    "чтоб обязательно была звезда! —\n",
    "клянется —\n",
    "не перенесет эту беззвездную муку!\n",
    "А после\n",
    "ходит тревожный,\n",
    "но спокойный наружно.\n",
    "Говорит кому-то:\n",
    "«Ведь теперь тебе ничего?\n",
    "Не страшно?\n",
    "Да?!»\n",
    "Послушайте!\n",
    "Ведь, если звезды\n",
    "зажигают —\n",
    "значит — это кому-нибудь нужно?\n",
    "Значит — это необходимо,\n",
    "чтобы каждый вечер\n",
    "над крышами\n",
    "загоралась хоть одна звезда?!\n",
    "'''\n",
    "\n",
    "Lermontov = '''\n",
    "Сквозь волнистые туманы\n",
    "Пробирается луна,\n",
    "На печальные поляны\n",
    "Льет печально свет одна.\n",
    "По дороге зимней, скучной\n",
    "Тройка борзая бежит,\n",
    "Колокольчик однозвучный\n",
    "Утомительно гремит.\n",
    "Что-то слышится родное\n",
    "В долгих песнях ямщика:\n",
    "То разгулье удалое,\n",
    "То сердечная тоска.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки Gensim (dictionary, corpus, tf-idf, LDA, word2vec), spacy (лемматизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможности библиотеки:\n",
    "+ создание словаря и корпуса по заданному документу\n",
    "+ создание биграмм и триграмм\n",
    "+ создание матрицы TFIDF\n",
    "+ создание тематических моделей/выделение ключевых слов\n",
    "+ векторизация слов и предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание словаря по заданному документу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_text.txt', 'r') as file:\n",
    "    sample_text = file.read()\n",
    "sample_text = nltk.sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "def prepare_text(text):\n",
    "    return [remove_stopwords(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \\\n",
    "               for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = prepare_text(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "sample_dictionary = corpora.Dictionary(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_dictionary)\n",
    "print(sample_dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание корпуса по заданному документу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_corpus = [\"Help me get my feet back on the ground\",\n",
    "                      \"Won't you please, please help me?\",\n",
    "                      \"Help me? Help me?\"]\n",
    "sample_text_corpus = [simple_preprocess(sent) for sent in sample_text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dictionary_corpus = corpora.Dictionary(sample_text_corpus)\n",
    "sample_corpus = [sample_dictionary_corpus.doc2bow(sent, allow_update=True) for sent in sample_text_corpus]\n",
    "sample_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decipher(dictionary, corpus):\n",
    "    return [[(dictionary[num], round(count, 2)) for num, count in line] for line in corpus]\n",
    "\n",
    "decipher(sample_dictionary_corpus, sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение и импорт словарей и корпусов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dictionary.save('sample_dictionary.dict')\n",
    "loaded_sample_dictionary = corpora.Dictionary.load('sample_dictionary.dict')\n",
    "print(loaded_sample_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('sample_corpus.mm', sample_corpus)\n",
    "loaded_sample_corpus = [[(num, int(count))for num, count in line] for line in corpora.MmCorpus('sample_corpus.mm')]\n",
    "loaded_sample_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание биграмм и триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.phrases.Phrases(sample_text, min_count=3, threshold=10)\n",
    "sample_text_bigram = [bigram[sent] for sent in sample_text]\n",
    "print(sample_text_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание: добиться того, чтобы \"how are you\" выделилось как триграмма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ понять как делаются триграммы\n",
    "+ обработать текст (лучше только с использованием simple_preprocess, чтобы ничего не удалилось, если оно входит в stopwords)\n",
    "+ создать модель триграммы\n",
    "+ получить текст с выделенными триграммами (how are you -> how_are_you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text = \"\"\"\n",
    "- Hi Jacob, how are you? \n",
    "- I'm fine, Nathan, how are you, Joseph? \n",
    "- I'm all good, Jacob, how are you, Natalie? \n",
    "- I'm okay, Joseph, how are you, Nikolas?\n",
    "- Nothing is wrong, Natalie, how are you, Grian?\n",
    "- Everything is pretty good, Nikolas,  how are you, Oliver?\n",
    "- Fantastic, Grian! How are you, Mathew?\n",
    "- It's never been better, Oliver, how are you, Minerva?\n",
    "- Great as always, Mathew, how are you, Oleg?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (TF — term frequency, IDF — inverse document frequency): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decipher(sample_dictionary_corpus, sample_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tfidf = models.TfidfModel(sample_corpus, smartirs='ntc')\n",
    "print(decipher(sample_dictionary_corpus, sample_tfidf[sample_corpus]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт наборов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не рекомендуется запускать просто так\n",
    "dataset = api.load(\"text8\")\n",
    "data = [i for i in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация в spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_text_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "sample_lemmatized = [[token.lemma_ for token in nlp(\" \".join(sentence)) \\\n",
    "                      if token.pos_ in allowed_postags] for sentence in sample_text_bigram]\n",
    "print(sample_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тематических моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_dictionary = corpora.Dictionary(sample_lemmatized)\n",
    "LDA_corpus = [LDA_dictionary.doc2bow(sent) for sent in sample_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_model = LdaMulticore(corpus=LDA_corpus,\n",
    "                         id2word=LDA_dictionary,\n",
    "                         num_topics=1, #количество тем\n",
    "                         passes=10,\n",
    "                         chunksize=5, # количество подсписков используемых в каждом проходе\n",
    "                         iterations=100, \n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "LDA_model.save('LDA_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание: используя файл politics.txt создать тематическую модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        + подготовить файл к обработке\n",
    "        + найти биграммы (если очень хочется, то и триграммы)\n",
    "        + лемматизировать\n",
    "        + создать словарь\n",
    "        + создать корпус\n",
    "        + создать тематическую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение ключевых слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_text.txt', 'r') as file:\n",
    "    just_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intel is developing ultra-wideband technology (UWB) which would allow fast data transfer but with low power needs.'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(just_text, word_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n",
      "technologies\n",
      "uwb\n",
      "intel\n",
      "data\n",
      "ultra fast\n",
      "usb\n",
      "phone conference\n",
      "groups\n",
      "group\n",
      "consumer electronics\n",
      "samsung\n",
      "use\n",
      "firm\n",
      "uses\n",
      "huge\n",
      "power\n",
      "people beam\n",
      "short\n",
      "radio\n"
     ]
    }
   ],
   "source": [
    "print(keywords(just_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('politics.txt', 'r') as file:\n",
    "    politics_w2v = file.read()\n",
    "politics_w2v = prepare_text(nltk.sent_tokenize(politics_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_w2v_model = gensim.models.Word2Vec(politics_w2v, size = 500, window = 5, min_count=2, workers = 4, hs = 1, negative = 0)\n",
    "politics_w2v_model.save('politics_w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44228965\n",
      "0.82698345\n"
     ]
    }
   ],
   "source": [
    "print(politics_w2v_model.wv.similarity('britain','china'))\n",
    "print(politics_w2v_model.wv.similarity('war','china'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.936535], dtype=float32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics_w2v_model.score([\"British government\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'minister'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics_w2v_model.wv.doesnt_match(\"government king minister\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('called', 0.9408179521560669),\n",
       " ('idea', 0.9012004137039185),\n",
       " ('current', 0.8952766060829163),\n",
       " ('added', 0.889606237411499),\n",
       " ('lists', 0.8774770498275757)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics_w2v_model.wv.most_similar('government', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('threatened', 0.7175700664520264),\n",
       " ('least', 0.7130386829376221),\n",
       " ('july', 0.7118679285049438),\n",
       " ('suggests', 0.7083441615104675),\n",
       " ('allawi', 0.6986973285675049),\n",
       " ('policies', 0.698662281036377),\n",
       " ('bad', 0.6974447965621948),\n",
       " ('stevens', 0.6947340965270996),\n",
       " ('undergo', 0.6881623268127441),\n",
       " ('hague', 0.6794817447662354)]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics_w2v_model.wv.most_similar(positive=['king'], negative=['government'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model300 = api.load('word2vec-google-news-300') # если хочется действительно что-то интересное посмотреть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на случай, если вы решитесь скачать модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать предложение и заменить в нем все слова, кроме предлогов (использовать stopwords), на наиболее похожие согласно модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
