{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import pymorphy2\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "def prepare_text(text):\n",
    "    return [remove_stopwords(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \\\n",
    "               for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 'jacob', 'how_are_you'], ['fine', 'nathan', 'how_are_you', 'joseph'], ['all', 'good', 'jacob', 'how_are_you', 'natalie'], ['okay', 'joseph', 'how_are_you', 'nikolas'], ['nothing', 'is', 'wrong', 'natalie', 'how_are_you', 'grian'], ['everything', 'is', 'pretty', 'good', 'nikolas', 'how_are_you', 'oliver'], ['fantastic', 'grian'], ['how_are_you', 'mathew'], ['it', 'never', 'been', 'better', 'oliver', 'how_are_you', 'minerva'], ['great', 'as', 'always', 'mathew', 'how_are_you', 'oleg']]\n"
     ]
    }
   ],
   "source": [
    "task_text = \"\"\"\n",
    "- Hi Jacob, how are you? \n",
    "- I'm fine, Nathan, how are you, Joseph? \n",
    "- I'm all good, Jacob, how are you, Natalie? \n",
    "- I'm okay, Joseph, how are you, Nikolas?\n",
    "- Nothing is wrong, Natalie, how are you, Grian?\n",
    "- Everything is pretty good, Nikolas,  how are you, Oliver?\n",
    "- Fantastic, Grian! How are you, Mathew?\n",
    "- It's never been better, Oliver, how are you, Minerva?\n",
    "- Great as always, Mathew, how are you, Oleg?\n",
    "\"\"\"\n",
    "\n",
    "task_text = [gensim.utils.simple_preprocess(str(sentence), deacc=True) for sentence in nltk.sent_tokenize(task_text)]\n",
    "task_bigram = gensim.models.phrases.Phrases(task_text, min_count=3, threshold=4)\n",
    "task_trigram = gensim.models.phrases.Phrases(task_bigram[task_text], min_count=3, threshold=4)\n",
    "task_text = [task_trigram[task_bigram[sent]] for sent in task_text]\n",
    "print(task_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('politics.txt', 'r') as file:\n",
    "    politics_text = file.read()\n",
    "politics_text = nltk.sent_tokenize(politics_text)\n",
    "politics_text = prepare_text(politics_text)\n",
    "politics_bigram = gensim.models.phrases.Phrases(politics_text, min_count=3, threshold=10)\n",
    "politics_text_bigram = [politics_bigram[sent] for sent in politics_text]\n",
    "politics_allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "politics_lemmatized = [[token.lemma_ for token in nlp(\" \".join(sentence)) \\\n",
    "                      if token.pos_ in politics_allowed_postags] for sentence in politics_text_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_LDA_dictionary = corpora.Dictionary(politics_lemmatized)\n",
    "politics_LDA_corpus = [politics_LDA_dictionary.doc2bow(sent) for sent in politics_lemmatized]\n",
    "politics_LDA_model = LdaMulticore(corpus=politics_LDA_corpus,\n",
    "                         id2word=politics_LDA_dictionary,\n",
    "                         num_topics=20, #количество тем\n",
    "                         passes=10,\n",
    "                         chunksize=100, # количество подсписков используемых в каждом проходе\n",
    "                         iterations=100, \n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "politics_LDA_model.save('politics_LDA_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.091*\"police\" + 0.078*\"call\" + 0.041*\"hold\" + 0.037*\"action\" + 0.030*\"still\" + 0.027*\"figure\" + 0.026*\"measure\" + 0.025*\"rise\" + 0.024*\"aid\" + 0.021*\"debate\"'),\n",
       " (1,\n",
       "  '0.074*\"say\" + 0.037*\"see\" + 0.035*\"want\" + 0.030*\"leader\" + 0.030*\"return\" + 0.027*\"government\" + 0.021*\"give\" + 0.020*\"however\" + 0.019*\"meet\" + 0.019*\"change\"'),\n",
       " (2,\n",
       "  '0.068*\"say\" + 0.036*\"problem\" + 0.031*\"end\" + 0.025*\"hope\" + 0.025*\"long\" + 0.023*\"service\" + 0.021*\"follow\" + 0.021*\"limit\" + 0.020*\"much\" + 0.017*\"live\"'),\n",
       " (3,\n",
       "  '0.077*\"election\" + 0.048*\"school\" + 0.032*\"go\" + 0.030*\"happen\" + 0.025*\"say\" + 0.022*\"back\" + 0.021*\"vote\" + 0.021*\"comment\" + 0.020*\"state\" + 0.019*\"labour\"'),\n",
       " (4,\n",
       "  '0.053*\"year\" + 0.038*\"say\" + 0.033*\"world\" + 0.033*\"question\" + 0.032*\"concern\" + 0.032*\"political\" + 0.025*\"number\" + 0.023*\"run\" + 0.021*\"home\" + 0.019*\"casino\"'),\n",
       " (5,\n",
       "  '0.066*\"think\" + 0.045*\"ask\" + 0.042*\"people\" + 0.042*\"come\" + 0.036*\"tell\" + 0.035*\"game\" + 0.035*\"force\" + 0.028*\"say\" + 0.024*\"well\" + 0.023*\"thing\"'),\n",
       " (6,\n",
       "  '0.039*\"say\" + 0.028*\"talk\" + 0.026*\"become\" + 0.024*\"support\" + 0.024*\"wrong\" + 0.020*\"urge\" + 0.020*\"break\" + 0.019*\"people\" + 0.017*\"insist\" + 0.017*\"point\"'),\n",
       " (7,\n",
       "  '0.047*\"court\" + 0.039*\"order\" + 0.034*\"mean\" + 0.033*\"agree\" + 0.032*\"say\" + 0.023*\"suspect\" + 0.020*\"real\" + 0.020*\"keep\" + 0.019*\"join\" + 0.018*\"send\"'),\n",
       " (8,\n",
       "  '0.052*\"system\" + 0.051*\"legal\" + 0.049*\"community\" + 0.045*\"cost\" + 0.043*\"fail\" + 0.036*\"involve\" + 0.036*\"common\" + 0.025*\"later\" + 0.023*\"describe\" + 0.021*\"meeting\"'),\n",
       " (9,\n",
       "  '0.071*\"say\" + 0.035*\"believe\" + 0.032*\"public\" + 0.029*\"way\" + 0.024*\"argue\" + 0.023*\"look\" + 0.019*\"country\" + 0.018*\"try\" + 0.018*\"people\" + 0.017*\"campaign\"'),\n",
       " (10,\n",
       "  '0.074*\"need\" + 0.062*\"also\" + 0.046*\"british\" + 0.040*\"day\" + 0.033*\"lead\" + 0.028*\"say\" + 0.026*\"spend\" + 0.025*\"programme\" + 0.022*\"money\" + 0.021*\"key\"'),\n",
       " (11,\n",
       "  '0.029*\"member\" + 0.025*\"tackle\" + 0.025*\"say\" + 0.022*\"abuse\" + 0.020*\"wait\" + 0.019*\"show\" + 0.019*\"line\" + 0.018*\"away\" + 0.018*\"intend\" + 0.017*\"family\"'),\n",
       " (12,\n",
       "  '0.075*\"say\" + 0.055*\"law\" + 0.036*\"issue\" + 0.034*\"child\" + 0.034*\"man\" + 0.033*\"tory\" + 0.029*\"bill\" + 0.024*\"face\" + 0.023*\"government\" + 0.020*\"part\"'),\n",
       " (13,\n",
       "  '0.064*\"say\" + 0.049*\"deal\" + 0.049*\"hunt\" + 0.033*\"even\" + 0.032*\"set\" + 0.031*\"ensure\" + 0.029*\"sport\" + 0.028*\"far\" + 0.024*\"great\" + 0.022*\"feel\"'),\n",
       " (14,\n",
       "  '0.054*\"say\" + 0.038*\"case\" + 0.028*\"claim\" + 0.024*\"time\" + 0.023*\"power\" + 0.020*\"spokesman\" + 0.019*\"prime_minister\" + 0.017*\"high\" + 0.016*\"human_right\" + 0.016*\"leave\"'),\n",
       " (15,\n",
       "  '0.095*\"say\" + 0.032*\"move\" + 0.024*\"former\" + 0.024*\"minister\" + 0.021*\"charge\" + 0.020*\"country\" + 0.019*\"government\" + 0.016*\"introduce\" + 0.016*\"accept\" + 0.014*\"people\"'),\n",
       " (16,\n",
       "  '0.045*\"report\" + 0.043*\"say\" + 0.043*\"vote\" + 0.035*\"accuse\" + 0.035*\"area\" + 0.032*\"close\" + 0.031*\"target\" + 0.026*\"last\" + 0.025*\"sentence\" + 0.024*\"many\"'),\n",
       " (17,\n",
       "  '0.044*\"job\" + 0.043*\"deny\" + 0.041*\"student\" + 0.041*\"find\" + 0.038*\"work\" + 0.038*\"pay\" + 0.031*\"help\" + 0.030*\"get\" + 0.028*\"provide\" + 0.026*\"evidence\"'),\n",
       " (18,\n",
       "  '0.053*\"allow\" + 0.044*\"say\" + 0.029*\"prison\" + 0.028*\"make\" + 0.027*\"continue\" + 0.027*\"put\" + 0.026*\"clear\" + 0.025*\"consider\" + 0.023*\"add\" + 0.023*\"lose\"'),\n",
       " (19,\n",
       "  '0.061*\"say\" + 0.044*\"expect\" + 0.027*\"plan\" + 0.025*\"immigration\" + 0.024*\"take\" + 0.024*\"week\" + 0.023*\"place\" + 0.019*\"statement\" + 0.018*\"control\" + 0.016*\"pledge\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics_LDA_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In my head, a review about the M. Video store chain has long been ripening. Since the very time when I again faced with the problem of returning money for a certificate of the Additional Services Program. I already wrote a review about the work of Eldorado, in fact, faced with the same problem, but for the first time. True, in Eldorado this lure is called the Additional Service Program, but the difference is small. Actually, to this day I think that M-video is a bad store, but due to the small selection of those in the city, sometimes I still get something there. pah-pah, while it works. However, today I want to share my opinion about M. Video. I will tell you about all the problems of this place. This is the second largest electronics store that we have in the city. A wide range of products is presented here, although it is often impossible to find a specific model. The store has promotions, discounts, a system of accumulative bonuses, credit purchases and other rubbish, which is called a service. However, there are many problems with all promotions. Once again, we got into such a bad situation. Firstly, the service in the store was terrible. And the quality of the goods was bad. Two years ago, they threw themselves off, threw themselves in and presented mom with a new TV set that had broken. It was very disappointing that the TV broke down, since we paid a huge amount for it. Not a single visit ended without a problem. However, when they bought Dad, they again drew this one and imposed the acquisition of a certificate of the Additional Services Program. So roughly (this is a copy) this stucco looks and costs decent money. The attraction is that at the end of the certificate validity period, money for unused services will be returned. But all this lies, with a certificate you get even more problems! Do not believe in stocks and do not go to stores with a bad reputation, do not create problems for yourself!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 6), ('store', 5), ('certificate', 4), ('bad', 4), ('problems', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [i for i in nltk.word_tokenize(text) if i not in string.punctuation and i not in stopwords.words('english') ]\n",
    "nltk.FreqDist(t).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pushkin = '''\n",
    "Мороз и солнце; день чудесный!\n",
    "Еще ты дремлешь, друг прелестный —\n",
    "Пора, красавица, проснись:\n",
    "Открой сомкнуты негой взоры\n",
    "Навстречу северной Авроры,\n",
    "Звездою севера явись!\n",
    "\n",
    "Вечор, ты помнишь, вьюга злилась,\n",
    "На мутном небе мгла носилась;\n",
    "Луна, как бледное пятно,\n",
    "Сквозь тучи мрачные желтела,\n",
    "И ты печальная сидела —\n",
    "А нынче погляди в окно:\n",
    "\n",
    "Под голубыми небесами\n",
    "Великолепными коврами,\n",
    "Блестя на солнце, снег лежит;\n",
    "Прозрачный лес один чернеет,\n",
    "И ель сквозь иней зеленеет,\n",
    "И речка подо льдом блестит.\n",
    "'''\n",
    "\n",
    "Mayakovsky = '''\n",
    "Послушайте!\n",
    "Ведь, если звезды зажигают —\n",
    "значит — это кому-нибудь нужно?\n",
    "Значит — кто-то хочет, чтобы они были?\n",
    "Значит — кто-то называет эти плевочки\n",
    "\n",
    "жемчужиной?\n",
    "И, надрываясь\n",
    "в метелях полуденной пыли,\n",
    "врывается к богу,\n",
    "боится, что опоздал,\n",
    "плачет,\n",
    "целует ему жилистую руку,\n",
    "просит —\n",
    "чтоб обязательно была звезда! —\n",
    "клянется —\n",
    "не перенесет эту беззвездную муку!\n",
    "А после\n",
    "ходит тревожный,\n",
    "но спокойный наружно.\n",
    "Говорит кому-то:\n",
    "«Ведь теперь тебе ничего?\n",
    "Не страшно?\n",
    "Да?!»\n",
    "Послушайте!\n",
    "Ведь, если звезды\n",
    "зажигают —\n",
    "значит — это кому-нибудь нужно?\n",
    "Значит — это необходимо,\n",
    "чтобы каждый вечер\n",
    "над крышами\n",
    "загоралась хоть одна звезда?!\n",
    "'''\n",
    "\n",
    "Lermontov = '''\n",
    "Сквозь волнистые туманы\n",
    "Пробирается луна,\n",
    "На печальные поляны\n",
    "Льет печально свет одна.\n",
    "По дороге зимней, скучной\n",
    "Тройка борзая бежит,\n",
    "Колокольчик однозвучный\n",
    "Утомительно гремит.\n",
    "Что-то слышится родное\n",
    "В долгих песнях ямщика:\n",
    "То разгулье удалое,\n",
    "То сердечная тоска.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_class(author):\n",
    "    words = [i for i in nltk.word_tokenize(author) if i not in string.punctuation and i not in stopwords.words('russian') ]\n",
    "    lst = []\n",
    "    for i in words:\n",
    "        word = morph.parse(i)[0]\n",
    "        lst.append(word.tag.POS) \n",
    "    return nltk.FreqDist(lst).most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ADJF', 11)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_class(Lermontov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
